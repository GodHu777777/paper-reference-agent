"""
é¡µç æå–å™¨
ä»ä¸åŒæ¥æºæå–é¡µç ä¿¡æ¯
"""
import re
import requests
from typing import Optional, Dict, Any
from bs4 import BeautifulSoup

import config
from .utils import normalize_pages


class PageExtractor:
    """é¡µç æå–å™¨åŸºç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': config.USER_AGENT})
        # åªåœ¨ä»£ç†é…ç½®ä¸ä¸ºç©ºä¸”æ˜¯æœ‰æ•ˆå­—å…¸æ—¶ä½¿ç”¨ä»£ç†
        if config.PROXIES and isinstance(config.PROXIES, dict) and config.PROXIES:
            try:
                if any(v for v in config.PROXIES.values() if v):
                    self.session.proxies.update(config.PROXIES)
            except Exception:
                self.session.proxies = {}
    
    def extract(self, paper_info: Dict[str, Any]) -> Optional[str]:
        """
        æå–é¡µç 
        
        Args:
            paper_info: è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            é¡µç å­—ç¬¦ä¸²ï¼Œå¦‚ "123-145"
        """
        raise NotImplementedError


class SemanticScholarExtractor(PageExtractor):
    """ä» Semantic Scholar API æå–é¡µç """
    
    def extract(self, paper_info: Dict[str, Any]) -> Optional[str]:
        """ä» Semantic Scholar è¿”å›çš„æ•°æ®ä¸­æå–é¡µç """
        # ç›´æ¥ä» API è¿”å›æ•°æ®ä¸­è·å–
        pages = paper_info.get('pages')
        if pages:
            return normalize_pages(pages)
        
        # å°è¯•ä»å…¶ä»–å­—æ®µè·å–
        if 'publicationVenue' in paper_info:
            venue = paper_info['publicationVenue']
            if isinstance(venue, dict) and 'pages' in venue:
                return normalize_pages(venue['pages'])
        
        return None


class DBLPExtractor(PageExtractor):
    """ä» DBLP æå–é¡µç """
    
    def __init__(self):
        super().__init__()
        # å»¶è¿Ÿå¯¼å…¥ LLM æå–å™¨ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
        self._llm_extractor = None
    
    def _get_llm_extractor(self):
        """è·å– LLM æå–å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._llm_extractor is None:
            try:
                from .llm_extractor import LLMExtractor
                self._llm_extractor = LLMExtractor()
            except ImportError:
                pass
        return self._llm_extractor
    
    def extract(self, paper_info: Dict[str, Any]) -> Optional[str]:
        """ä» DBLP æ•°æ®ä¸­æå–é¡µç """
        pages = paper_info.get('pages')
        if pages:
            return normalize_pages(pages)
        
        # å¦‚æœæœ‰ DBLP URLï¼Œå°è¯•çˆ¬å–
        dblp_url = paper_info.get('dblp_url') or paper_info.get('url')
        if dblp_url and 'dblp.org' in dblp_url:
            # å…ˆå°è¯•ä¼ ç»Ÿæ–¹æ³•
            pages = self._fetch_from_dblp_page(dblp_url)
            if pages:
                return pages
            
            # å¦‚æœä¼ ç»Ÿæ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ LLM æå–
            llm_extractor = self._get_llm_extractor()
            if llm_extractor:
                print("  ä½¿ç”¨ LLM ä»ç½‘é¡µæå–é¡µç ...")
                paper_title = paper_info.get('title', '')
                pages = llm_extractor.extract_from_url(dblp_url, paper_title)
                if pages:
                    return pages
        
        return None
    
    def _fetch_from_dblp_page(self, url: str) -> Optional[str]:
        """ä» DBLP é¡µé¢çˆ¬å–é¡µç """
        try:
            response = self.session.get(url, timeout=config.REQUEST_TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            
            # æŸ¥æ‰¾é¡µç ä¿¡æ¯
            # DBLP é€šå¸¸åœ¨ <span class="pages"> æ ‡ç­¾ä¸­
            pages_elem = soup.find('span', class_='pages')
            if pages_elem:
                return normalize_pages(pages_elem.get_text())
            
            # æˆ–è€…åœ¨ cite æ ‡ç­¾ä¸­
            cite_elem = soup.find('cite', {'itemprop': 'pagination'})
            if cite_elem:
                return normalize_pages(cite_elem.get_text())
            
        except Exception as e:
            print(f"ä» DBLP é¡µé¢æå–é¡µç å¤±è´¥: {e}")
        
        return None


class CrossRefExtractor(PageExtractor):
    """ä» CrossRef API æå–é¡µç """
    
    def extract(self, paper_info: Dict[str, Any]) -> Optional[str]:
        """ä» CrossRef æ•°æ®ä¸­æå–é¡µç """
        pages = paper_info.get('pages')
        if pages:
            return normalize_pages(pages)
        
        # CrossRef çš„é¡µç å¯èƒ½åœ¨ 'page' å­—æ®µ
        if 'page' in paper_info:
            return normalize_pages(paper_info['page'])
        
        return None


class BibTeXExtractor(PageExtractor):
    """ä» BibTeX å­—ç¬¦ä¸²æå–é¡µç """
    
    def extract_from_bibtex(self, bibtex: str) -> Optional[str]:
        """
        ä» BibTeX å­—ç¬¦ä¸²æå–é¡µç 
        
        Args:
            bibtex: BibTeX æ ¼å¼çš„å­—ç¬¦ä¸²
            
        Returns:
            é¡µç å­—ç¬¦ä¸²
        """
        # ç®€å•çš„æ­£åˆ™åŒ¹é… pages å­—æ®µ
        match = re.search(r'pages\s*=\s*[{\"]([^}\"]+)[}\"]', bibtex, re.IGNORECASE)
        if match:
            return normalize_pages(match.group(1))
        
        return None
    
    def extract_volume_issue_from_bibtex(self, bibtex: str) -> Dict[str, Optional[str]]:
        """
        ä» BibTeX å­—ç¬¦ä¸²æå–å·æœŸå·
        
        Args:
            bibtex: BibTeX æ ¼å¼çš„å­—ç¬¦ä¸²
            
        Returns:
            åŒ…å« volume å’Œ issue çš„å­—å…¸
        """
        result = {'volume': None, 'issue': None}
        
        # æå– volume
        volume_match = re.search(r'volume\s*=\s*[{\"]([^}\"]+)[}\"]', bibtex, re.IGNORECASE)
        if volume_match:
            result['volume'] = volume_match.group(1).strip()
        
        # æå– issue æˆ– number
        issue_match = re.search(r'(?:issue|number)\s*=\s*[{\"]([^}\"]+)[}\"]', bibtex, re.IGNORECASE)
        if issue_match:
            result['issue'] = issue_match.group(1).strip()
        
        return result


class DOI2BibExtractor(PageExtractor):
    """ä½¿ç”¨ doi2bib å‘½ä»¤è¡Œå·¥å…·æå–é¡µç ï¼ˆé€šè¿‡ DOI è·å– BibTeXï¼‰"""
    
    def extract_from_doi(self, doi: str) -> Optional[str]:
        """
        ä» DOI é€šè¿‡ doi2bib å‘½ä»¤è¡Œå·¥å…·è·å– BibTeX å¹¶æå–é¡µç 
        
        Args:
            doi: DOI æ ‡è¯†ç¬¦ï¼ˆä¾‹å¦‚ï¼š10.1016/j.trc.2015.04.007ï¼‰
            
        Returns:
            é¡µç å­—ç¬¦ä¸²
        """
        import subprocess
        import sys
        import os
        
        try:
            print(f"    â†’ ä½¿ç”¨ doi2bib è·å– BibTeX...")
            print(f"    â†’ DOI: {doi}")
            
            bibtex = None
            
            # æ–¹æ³•1: å°è¯•ç›´æ¥ä½¿ç”¨ Python APIï¼ˆdoi2bib.crossrefï¼‰
            try:
                import doi2bib.crossref as d2b_crossref
                print(f"    â†’ å°è¯•ä½¿ç”¨ doi2bib.crossref Python API...")
                found, bibtex_result = d2b_crossref.get_bib(doi)
                if found and bibtex_result:
                    bibtex = bibtex_result.strip()
                    print(f"    â†’ é€šè¿‡ Python API è·å–åˆ° BibTeXï¼Œé•¿åº¦: {len(bibtex)} å­—ç¬¦")
            except ImportError:
                if config.DEBUG:
                    print(f"    [DEBUG] doi2bib.crossref æ¨¡å—ä¸å¯ç”¨")
            except Exception as e:
                if config.DEBUG:
                    print(f"    [DEBUG] Python API è°ƒç”¨å¤±è´¥: {e}")
            
            # æ–¹æ³•2: å¦‚æœ Python API å¤±è´¥ï¼Œå°è¯•å‘½ä»¤è¡Œå·¥å…·ï¼ˆæ¨¡æ‹Ÿå‘½ä»¤è¡Œæ‰§è¡Œï¼‰
            if not bibtex:
                print(f"    â†’ Python API ä¸å¯ç”¨ï¼Œå°è¯•å‘½ä»¤è¡Œå·¥å…·...")
                commands_to_try = [
                    ['doi2bib', doi],  # ç›´æ¥å‘½ä»¤
                    [sys.executable, '-m', 'doi2bib', doi],  # ä½¿ç”¨å½“å‰ Python è§£é‡Šå™¨
                    ['python', '-m', 'doi2bib', doi],  # Python æ¨¡å—æ–¹å¼ï¼ˆå¤‡ç”¨ï¼‰
                ]
                
                for cmd in commands_to_try:
                    try:
                        print(f"    â†’ å°è¯•æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
                        
                        result = subprocess.run(
                            cmd,
                            capture_output=True,
                            text=True,
                            timeout=config.REQUEST_TIMEOUT,
                            check=False  # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œæˆ‘ä»¬è‡ªå·±å¤„ç†
                        )
                        
                        if config.DEBUG:
                            print(f"    [DEBUG] è¿”å›ç : {result.returncode}")
                            if result.stdout:
                                print(f"    [DEBUG] æ ‡å‡†è¾“å‡ºå‰200å­—ç¬¦: {result.stdout.strip()[:200]}")
                            if result.stderr:
                                print(f"    [DEBUG] æ ‡å‡†é”™è¯¯: {result.stderr.strip()[:200]}")
                        
                        # å¦‚æœå‘½ä»¤æˆåŠŸï¼ˆè¿”å›ç ä¸º 0ï¼‰ä¸”æœ‰è¾“å‡º
                        if result.returncode == 0 and result.stdout.strip():
                            bibtex = result.stdout.strip()
                            print(f"    â†’ é€šè¿‡å‘½ä»¤è¡Œè·å–åˆ° BibTeXï¼Œé•¿åº¦: {len(bibtex)} å­—ç¬¦")
                            break
                        elif result.returncode != 0:
                            if config.DEBUG:
                                print(f"    â†’ å‘½ä»¤å¤±è´¥ (è¿”å›ç : {result.returncode})")
                            continue
                            
                    except FileNotFoundError:
                        # å‘½ä»¤ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
                        if config.DEBUG:
                            print(f"    [DEBUG] å‘½ä»¤æœªæ‰¾åˆ°: {' '.join(cmd)}")
                        continue
                    except subprocess.TimeoutExpired:
                        print(f"    âš  doi2bib å‘½ä»¤è¶…æ—¶")
                        continue
                    except Exception as e:
                        if config.DEBUG:
                            print(f"    [DEBUG] æ‰§è¡Œå‘½ä»¤æ—¶å‡ºé”™: {e}")
                        continue
            
            if not bibtex:
                print(f"    âš  æ‰€æœ‰æ–¹æ³•å°è¯•å‡å¤±è´¥")
                print(f"    ğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²å®‰è£… doi2bib å·¥å…·: pip install doi2bib")
                print(f"    ğŸ’¡ æˆ–è€…æ£€æŸ¥æ˜¯å¦å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œç›´æ¥è¿è¡Œ: doi2bib {doi}")
                return None
            
            # æ˜¾ç¤º BibTeX é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰
            if config.DEBUG:
                print(f"    [DEBUG] BibTeX é¢„è§ˆ: {bibtex[:200]}...")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯ä¿¡æ¯
            if 'error' in bibtex.lower() or 'not found' in bibtex.lower() or 'invalid' in bibtex.lower():
                print(f"    âš  doi2bib è¿”å›é”™è¯¯ä¿¡æ¯")
                if config.DEBUG:
                    print(f"    [DEBUG] å®Œæ•´å“åº”: {bibtex}")
                return None
            
            # ä» BibTeX ä¸­æå–é¡µç 
            print(f"    â†’ å¼€å§‹ä» BibTeX ä¸­æå–é¡µç ...")
            bibtex_extractor = BibTeXExtractor()
            pages = bibtex_extractor.extract_from_bibtex(bibtex)
            
            if pages:
                print(f"    âœ“ æˆåŠŸæå–é¡µç : {pages}")
                return pages
            
            print(f"    âš  BibTeX ä¸­æœªæ‰¾åˆ°é¡µç å­—æ®µ")
            
            # å¦‚æœå¯ç”¨è°ƒè¯•ï¼Œæ˜¾ç¤º BibTeX å†…å®¹ä»¥ä¾¿æ’æŸ¥
            if config.DEBUG:
                print(f"    [DEBUG] å®Œæ•´ BibTeX å†…å®¹:")
                print(f"    {bibtex}")
            
            return None
            
        except Exception as e:
            print(f"    âš  ä» doi2bib æå–é¡µç å¤±è´¥: {e}")
            if config.DEBUG:
                import traceback
                print(f"    [DEBUG] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None


class PDFMetadataExtractor(PageExtractor):
    """ä» PDF å…ƒæ•°æ®æå–é¡µç ï¼ˆéœ€è¦ä¸‹è½½ PDFï¼‰"""
    
    def extract_from_pdf_url(self, pdf_url: str) -> Optional[Dict[str, Any]]:
        """
        ä» PDF URL æå–å…ƒæ•°æ®
        æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•éœ€è¦ä¸‹è½½ PDFï¼Œå¯èƒ½è¾ƒæ…¢
        
        Args:
            pdf_url: PDF æ–‡ä»¶çš„ URL
            
        Returns:
            åŒ…å«å…ƒæ•°æ®çš„å­—å…¸
        """
        # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦ PyPDF2 æˆ– pdfplumber
        # ç”±äºä¸‹è½½å’Œè§£æ PDF è¾ƒæ…¢ï¼Œå»ºè®®ä½œä¸ºåå¤‡æ–¹æ¡ˆ
        
        try:
            # é¦–å…ˆå°è¯•ä» PDF å“åº”å¤´è·å–é¡µæ•°
            response = self.session.head(pdf_url, timeout=config.REQUEST_TIMEOUT)
            content_length = response.headers.get('content-length')
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„ PDF è§£æé€»è¾‘
            # ä½†éœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ–åŒ…
            
            return None
            
        except Exception as e:
            print(f"ä» PDF æå–å…ƒæ•°æ®å¤±è´¥: {e}")
            return None


def extract_pages(paper_info: Dict[str, Any], source: str = 'auto') -> Optional[str]:
    """
    æ™ºèƒ½æå–é¡µç 
    
    Args:
        paper_info: è®ºæ–‡ä¿¡æ¯å­—å…¸
        source: æ•°æ®æºç±»å‹ ('semantic_scholar', 'dblp', 'crossref', 'auto')
        
    Returns:
        é¡µç å­—ç¬¦ä¸²
    """
    # æ£€æŸ¥æ˜¯å¦æ˜¯ NeurIPS URL
    url = paper_info.get('url') or paper_info.get('dblp_url')
    if url and ('neurips.cc' in url.lower() or 'nips.cc' in url.lower()):
        # ä½¿ç”¨ NeurIPS ç‰¹å®šçš„æå–å™¨
        try:
            from .neurips_extractor import NeurIPSExtractor
            neurips_extractor = NeurIPSExtractor()
            paper_title = paper_info.get('title', '')
            pages = neurips_extractor.extract_from_url(url, paper_title)
            if pages:
                return pages
        except ImportError:
            pass
    
    extractors = {
        'semantic_scholar': SemanticScholarExtractor(),
        'dblp': DBLPExtractor(),
        'crossref': CrossRefExtractor(),
    }
    
    if source != 'auto' and source in extractors:
        # ä½¿ç”¨æŒ‡å®šçš„æå–å™¨
        return extractors[source].extract(paper_info)
    
    # è‡ªåŠ¨æ¨¡å¼ï¼šå°è¯•æ‰€æœ‰æå–å™¨
    for extractor in extractors.values():
        pages = extractor.extract(paper_info)
        if pages:
            return pages
    
    return None

