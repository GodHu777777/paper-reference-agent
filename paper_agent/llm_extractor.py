"""
ä½¿ç”¨å¤§æ¨¡å‹ API ä»ç½‘é¡µå†…å®¹æå–é¡µç ä¿¡æ¯
"""
import requests
import json
import re
from typing import Optional, Dict, Any
from bs4 import BeautifulSoup

import llm_config
import config
from .utils import normalize_pages

# å°è¯•å¯¼å…¥ cloudscraperï¼ˆç”¨äºç»•è¿‡ Cloudflare ä¿æŠ¤ï¼‰
try:
    import cloudscraper
    CLOUDSCRAPER_AVAILABLE = True
except ImportError:
    CLOUDSCRAPER_AVAILABLE = False

# å°è¯•å¯¼å…¥ Seleniumï¼ˆç”¨äºæ‰§è¡Œ JavaScriptï¼Œç»•è¿‡ Cloudflareï¼‰
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service as ChromeService
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from selenium.webdriver.firefox.options import Options as FirefoxOptions
    from selenium.webdriver.firefox.service import Service as FirefoxService
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    from webdriver_manager.chrome import ChromeDriverManager
    from webdriver_manager.firefox import GeckoDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False


class LLMExtractor:
    """ä½¿ç”¨å¤§æ¨¡å‹ä»ç½‘é¡µå†…å®¹æå–é¡µç """
    
    def __init__(self):
        # API è¯·æ±‚ç”¨çš„ sessionï¼ˆç”¨äºè°ƒç”¨ LLM APIï¼‰
        self.api_session = requests.Session()
        # åªåœ¨ LLM ä»£ç†é…ç½®ä¸ä¸ºç©ºæ—¶ä½¿ç”¨
        if llm_config.PROXIES and isinstance(llm_config.PROXIES, dict) and llm_config.PROXIES:
            try:
                if any(v for v in llm_config.PROXIES.values() if v):
                    self.api_session.proxies.update(llm_config.PROXIES)
            except Exception:
                self.api_session.proxies = {}
        
        # è®¾ç½® API Keyï¼ˆå¦‚æœæœ‰ï¼‰
        if llm_config.API_KEY:
            self.api_session.headers.update({
                'Authorization': f'Bearer {llm_config.API_KEY}'
            })
        
        self.api_session.headers.update({
            'Content-Type': 'application/json',
        })
        
        # ç½‘é¡µè®¿é—®ç”¨çš„ sessionï¼ˆä½¿ç”¨ config.PROXIESï¼‰
        # å¯¹äºå— Cloudflare ä¿æŠ¤çš„ç½‘ç«™ï¼ˆå¦‚ ACMï¼‰ï¼Œä½¿ç”¨ cloudscraper
        if CLOUDSCRAPER_AVAILABLE:
            # ä½¿ç”¨ cloudscraper åˆ›å»º sessionï¼ˆå¯ä»¥ç»•è¿‡ Cloudflareï¼‰
            self.web_session = cloudscraper.create_scraper(
                browser={
                    'browser': 'chrome',
                    'platform': 'windows',
                    'desktop': True
                }
            )
            # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœé…ç½®äº†ä¸”æœ‰æ•ˆï¼‰
            if config.PROXIES and isinstance(config.PROXIES, dict) and config.PROXIES:
                try:
                    if any(v for v in config.PROXIES.values() if v):
                        self.web_session.proxies.update(config.PROXIES)
                except Exception:
                    self.web_session.proxies = {}
        else:
            # å¦‚æœæ²¡æœ‰ cloudscraperï¼Œä½¿ç”¨æ™®é€š requests
            self.web_session = requests.Session()
            # è®¾ç½®æ›´çœŸå®çš„æµè§ˆå™¨è¯·æ±‚å¤´ï¼Œé¿å…è¢«ç½‘ç«™é˜»æ­¢
            self.web_session.headers.update({
                'User-Agent': config.USER_AGENT,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
            })
            # åªæœ‰å½“ PROXIES ä¸ä¸ºç©ºä¸”æœ‰æ•ˆæ—¶æ‰è®¾ç½®ä»£ç†
            if config.PROXIES and isinstance(config.PROXIES, dict) and config.PROXIES:
                try:
                    if any(v for v in config.PROXIES.values() if v):
                        self.web_session.proxies.update(config.PROXIES)
                except Exception:
                    self.web_session.proxies = {}
        
        # Selenium WebDriverï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.driver = None
    
    def extract_from_url(self, url: str, paper_title: str = None) -> Optional[str]:
        """
        ä» URL è·å–ç½‘é¡µå†…å®¹ï¼Œä½¿ç”¨å¤§æ¨¡å‹æå–é¡µç 
        
        Args:
            url: ç½‘é¡µ URL
            paper_title: è®ºæ–‡æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œå¸®åŠ©å¤§æ¨¡å‹ç†è§£ä¸Šä¸‹æ–‡ï¼‰
            
        Returns:
            é¡µç å­—ç¬¦ä¸²ï¼Œå¦‚ "123-145"ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        if not llm_config.ENABLE_LLM_EXTRACTION:
            return None
        
        try:
            # è·å–ç½‘é¡µå†…å®¹ï¼ˆä½¿ç”¨ web_sessionï¼Œä½¿ç”¨ config.PROXIESï¼‰
            response = self.web_session.get(
                url,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            # è§£æ HTML
            soup = BeautifulSoup(response.content, 'lxml')
            
            # æå–æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤è„šæœ¬å’Œæ ·å¼ï¼‰
            for script in soup(["script", "style", "meta", "link"]):
                script.decompose()
            
            text = soup.get_text(separator=' ', strip=True)
            
            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼ˆé¿å… token è¿‡å¤šï¼‰
            if len(text) > 8000:
                text = text[:8000] + "..."
            
            # ä½¿ç”¨å¤§æ¨¡å‹æå–é¡µç 
            return self._extract_with_llm(text, url, paper_title)
            
        except Exception as e:
            print(f"LLM æå–é¡µç å¤±è´¥ ({url}): {e}")
            return None
    
    def _extract_with_llm(self, webpage_text: str, url: str, 
                         paper_title: str = None) -> Optional[str]:
        """
        è°ƒç”¨å¤§æ¨¡å‹ API æå–é¡µç ä¿¡æ¯
        
        Args:
            webpage_text: ç½‘é¡µæ–‡æœ¬å†…å®¹
            url: ç½‘é¡µ URL
            paper_title: è®ºæ–‡æ ‡é¢˜
            
        Returns:
            é¡µç å­—ç¬¦ä¸²
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(webpage_text, url, paper_title)
            
            # è°ƒç”¨ API
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ä¿¡æ¯æå–åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ç½‘é¡µå†…å®¹ä¸­å‡†ç¡®æå–è®ºæ–‡çš„é¡µç èŒƒå›´ä¿¡æ¯ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]
            
            # æ„å»ºè¯·æ±‚
            api_url = f"{llm_config.BASE_URL}/chat/completions"
            
            payload = {
                "model": llm_config.MODEL_NAME,
                "messages": messages,
                "temperature": llm_config.TEMPERATURE,
                "max_tokens": llm_config.MAX_TOKENS,
            }
            
            # å‘é€è¯·æ±‚ï¼ˆä½¿ç”¨ api_sessionï¼Œä½¿ç”¨ llm_config.PROXIESï¼‰
            response = self.api_session.post(
                api_url,
                json=payload,
                timeout=llm_config.TIMEOUT
            )
            response.raise_for_status()
            
            # è§£æå“åº”
            result = response.json()
            
            # æå–å›å¤å†…å®¹
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content'].strip()
                
                # ä»å›å¤ä¸­æå–é¡µç 
                pages = self._parse_llm_response(content)
                
                if pages:
                    return normalize_pages(pages)
            
            return None
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 401:
                print(f"âš  LLM API è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key")
            elif e.response.status_code == 429:
                print(f"âš  LLM API é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•")
            else:
                print(f"âš  LLM API é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"âš  LLM è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _build_prompt(self, webpage_text: str, url: str, 
                     paper_title: str = None) -> str:
        """æ„å»ºæç¤ºè¯"""
        prompt = f"""è¯·ä»ä»¥ä¸‹ç½‘é¡µå†…å®¹ä¸­æå–è®ºæ–‡çš„é¡µç èŒƒå›´ä¿¡æ¯ã€‚

ç½‘é¡µ URL: {url}
"""
        
        if paper_title:
            prompt += f"è®ºæ–‡æ ‡é¢˜: {paper_title}\n"
        
        prompt += f"""
ç½‘é¡µå†…å®¹:
{webpage_text}

è¯·ä»”ç»†æŸ¥æ‰¾ä»¥ä¸‹ä¿¡æ¯ï¼š
1. é¡µç èŒƒå›´ï¼ˆå¦‚ "123-145", "pages 123-145", "pp. 123-145" ç­‰ï¼‰
2. ä¼šè®®æˆ–æœŸåˆŠçš„é¡µç ä¿¡æ¯
3. è®ºæ–‡åœ¨ä¼šè®®é›†ä¸­çš„é¡µç èŒƒå›´

å¦‚æœæ‰¾åˆ°äº†é¡µç ä¿¡æ¯ï¼Œè¯·åªè¿”å›é¡µç èŒƒå›´ï¼ˆæ ¼å¼ï¼šå¼€å§‹é¡µ-ç»“æŸé¡µï¼Œä¾‹å¦‚ "123-145"ï¼‰ã€‚
å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¯·åªè¿”å› "æœªæ‰¾åˆ°"ã€‚

åªè¿”å›é¡µç ä¿¡æ¯ï¼Œä¸è¦è¿”å›å…¶ä»–å†…å®¹ã€‚
"""
        return prompt
    
    def _parse_llm_response(self, response: str) -> Optional[str]:
        """
        è§£æå¤§æ¨¡å‹çš„å›å¤ï¼Œæå–é¡µç ä¿¡æ¯
        
        Args:
            response: å¤§æ¨¡å‹çš„å›å¤æ–‡æœ¬
            
        Returns:
            é¡µç å­—ç¬¦ä¸²ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        # ç§»é™¤å¸¸è§çš„å‰ç¼€å’Œè¯´æ˜æ–‡å­—
        response = response.lower().strip()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«"æœªæ‰¾åˆ°"æˆ–ç±»ä¼¼è¡¨è¿°
        if any(keyword in response for keyword in ['æœªæ‰¾åˆ°', 'not found', 'æ²¡æœ‰æ‰¾åˆ°', 'æ— ']):
            return None
        
        # å°è¯•æå–é¡µç æ¨¡å¼
        import re
        
        # åŒ¹é…å„ç§é¡µç æ ¼å¼
        patterns = [
            r'(\d+)\s*[-â€“â€”]\s*(\d+)',  # 123-145, 123â€“145, 123â€”145
            r'pages?\s*[:ï¼š]\s*(\d+)\s*[-â€“â€”]\s*(\d+)',  # pages: 123-145
            r'pp\.?\s*[:ï¼š]?\s*(\d+)\s*[-â€“â€”]\s*(\d+)',  # pp. 123-145
            r'é¡µç èŒƒå›´[ï¼š:]\s*(\d+)\s*[-â€“â€”]\s*(\d+)',  # é¡µç èŒƒå›´: 123-145
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response)
            if match:
                if len(match.groups()) >= 2:
                    return f"{match.group(1)}-{match.group(2)}"
        
        # å¦‚æœæ¨¡å¼åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç›´æ¥æå–æ•°å­—å¯¹
        numbers = re.findall(r'\d+', response)
        if len(numbers) >= 2:
            return f"{numbers[0]}-{numbers[1]}"
        
        return None
    
    def extract_from_html(self, html_content: str, url: str = None, 
                         paper_title: str = None) -> Optional[str]:
        """
        ç›´æ¥ä» HTML å†…å®¹æå–é¡µç ï¼ˆä¸é‡æ–°ä¸‹è½½ï¼‰
        
        Args:
            html_content: HTML å†…å®¹
            url: ç½‘é¡µ URLï¼ˆå¯é€‰ï¼‰
            paper_title: è®ºæ–‡æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            é¡µç å­—ç¬¦ä¸²
        """
        if not llm_config.ENABLE_LLM_EXTRACTION:
            return None
        
        try:
            # è§£æ HTML
            soup = BeautifulSoup(html_content, 'lxml')
            
            # æå–æ–‡æœ¬å†…å®¹
            for script in soup(["script", "style", "meta", "link"]):
                script.decompose()
            
            text = soup.get_text(separator=' ', strip=True)
            
            # é™åˆ¶æ–‡æœ¬é•¿åº¦
            if len(text) > 8000:
                text = text[:8000] + "..."
            
            # ä½¿ç”¨å¤§æ¨¡å‹æå–
            return self._extract_with_llm(text, url or "", paper_title)
            
        except Exception as e:
            print(f"LLM ä» HTML æå–é¡µç å¤±è´¥: {e}")
            return None
    
    def extract_from_doi_url(self, doi_url: str, paper_title: str = None) -> Optional[str]:
        """
        ä¸“é—¨ä» DOI URL æå–é¡µç 
        
        Args:
            doi_url: DOI URLï¼ˆä¾‹å¦‚ https://doi.org/10.18653/v1/2023.acl-long.782ï¼‰
            paper_title: è®ºæ–‡æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            é¡µç å­—ç¬¦ä¸²
        """
        if not llm_config.ENABLE_LLM_EXTRACTION:
            return None
        
        try:
            print(f"  æ­£åœ¨è®¿é—® DOI ç½‘é¡µ: {doi_url}")
            
            # æå– DOI æ ‡è¯†ç¬¦ï¼ˆä¾‹å¦‚ï¼š10.1145/3539618.3591695ï¼‰
            doi_match = re.search(r'doi\.org/([^/]+/?.*)', doi_url)
            if not doi_match:
                print(f"  âš  æ— æ³•ä» URL æå– DOI")
                return None
            
            doi_identifier = doi_match.group(1)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ ACM DOIï¼ˆå‰ç¼€ä¸º 10.1145ï¼‰
            if doi_identifier.startswith('10.1145/'):
                # ç›´æ¥æ„é€  ACM çš„ URLï¼Œé¿å…é€šè¿‡ doi.org é‡å®šå‘
                acm_url = f"https://dl.acm.org/doi/{doi_identifier}"
                if config.DEBUG:
                    print(f"  [DEBUG] æ£€æµ‹åˆ° ACM DOIï¼Œç›´æ¥è®¿é—®: {acm_url}")
                
                # é¦–å…ˆå°è¯•ä½¿ç”¨ Seleniumï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if config.USE_SELENIUM and SELENIUM_AVAILABLE:
                    print(f"  ä½¿ç”¨ Selenium è®¿é—® ACM ç½‘ç«™...")
                    html_content = self._extract_with_selenium(acm_url)
                    if html_content:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ Cloudflare æŒ‘æˆ˜é¡µé¢
                        if 'just a moment' in html_content.lower() or 'checking your browser' in html_content.lower():
                            print(f"  âš  ä»ç„¶é‡åˆ° Cloudflare ä¿æŠ¤ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨éªŒè¯")
                            return None
                        # ä½¿ç”¨ LLM æå–
                        return self._extract_from_html_with_llm(html_content, acm_url, paper_title)
                
                # å°è¯•ç›´æ¥è®¿é—® ACM URLï¼ˆä½¿ç”¨ cloudscraper æˆ– requestsï¼‰
                try:
                    headers = {}
                    if hasattr(self.web_session, 'headers'):
                        headers = self.web_session.headers.copy()
                    headers['Referer'] = 'https://www.google.com/'
                    headers['Origin'] = 'https://www.google.com'
                    
                    response = self.web_session.get(
                        acm_url,
                        timeout=config.REQUEST_TIMEOUT,
                        headers=headers,
                        allow_redirects=True
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Cloudflare ä¿æŠ¤
                    if response.status_code == 403 or ('just a moment' in response.text.lower()):
                        # å¦‚æœå¯ç”¨äº† Seleniumï¼Œå†å°è¯•ä¸€æ¬¡
                        if config.USE_SELENIUM and SELENIUM_AVAILABLE:
                            print(f"  æ™®é€šè¯·æ±‚å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ Selenium...")
                            html_content = self._extract_with_selenium(acm_url)
                            if html_content and 'just a moment' not in html_content.lower():
                                return self._extract_from_html_with_llm(html_content, acm_url, paper_title)
                        print(f"  âš  ACM ç½‘ç«™å— Cloudflare ä¿æŠ¤ï¼Œæ— æ³•è‡ªåŠ¨è®¿é—®")
                        print(f"  ğŸ’¡ æç¤º: å»ºè®®ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“ï¼ˆå¦‚ DBLPã€Google Scholarï¼‰è·å–é¡µç ")
                        return None
                    
                    response.raise_for_status()
                    
                    # è§£æ HTML
                    html_content = response.text
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Cloudflare æŒ‘æˆ˜é¡µé¢
                    if 'just a moment' in html_content.lower() or 'checking your browser' in html_content.lower():
                        # å¦‚æœå¯ç”¨äº† Seleniumï¼Œå†å°è¯•ä¸€æ¬¡
                        if config.USE_SELENIUM and SELENIUM_AVAILABLE:
                            print(f"  æ£€æµ‹åˆ° Cloudflare ä¿æŠ¤ï¼Œå°è¯•ä½¿ç”¨ Selenium...")
                            html_content = self._extract_with_selenium(acm_url)
                            if html_content and 'just a moment' not in html_content.lower():
                                return self._extract_from_html_with_llm(html_content, acm_url, paper_title)
                        print(f"  âš  ç½‘ç«™ä½¿ç”¨äº† Cloudflare ä¿æŠ¤ï¼Œæ— æ³•è‡ªåŠ¨è®¿é—®")
                        print(f"  ğŸ’¡ æç¤º: å»ºè®®ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“ï¼ˆå¦‚ DBLPã€Google Scholarï¼‰è·å–é¡µç ")
                        return None
                    
                    # ä½¿ç”¨ LLM æå–
                    return self._extract_from_html_with_llm(html_content, acm_url, paper_title)
                    
                except requests.exceptions.HTTPError as e:
                    if e.response.status_code == 403:
                        # å¦‚æœå¯ç”¨äº† Seleniumï¼Œå†å°è¯•ä¸€æ¬¡
                        if config.USE_SELENIUM and SELENIUM_AVAILABLE:
                            print(f"  é‡åˆ° 403 é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨ Selenium...")
                            html_content = self._extract_with_selenium(acm_url)
                            if html_content:
                                return self._extract_from_html_with_llm(html_content, acm_url, paper_title)
                        print(f"  âš  ACM ç½‘ç«™è®¿é—®è¢«æ‹’ç» (403)")
                        print(f"  ğŸ’¡ æç¤º: å»ºè®®ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“ï¼ˆå¦‚ DBLPã€Google Scholarï¼‰è·å–é¡µç ")
                    return None
                except Exception as e:
                    if config.DEBUG:
                        print(f"  [DEBUG] ç›´æ¥è®¿é—® ACM URL å¤±è´¥: {e}")
            
            # å¯¹äºå…¶ä»– DOIï¼Œä½¿ç”¨æ ‡å‡†çš„é‡å®šå‘å¤„ç†
            # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼šä¸è·Ÿéšé‡å®šå‘ï¼Œæ‰‹åŠ¨å¤„ç†
            response = self.web_session.get(
                doi_url,
                timeout=config.REQUEST_TIMEOUT,
                allow_redirects=False  # ä¸è‡ªåŠ¨è·Ÿéšé‡å®šå‘
            )
            
            final_url = None
            
            # å¤„ç†ä¸åŒçš„å“åº”æƒ…å†µ
            if response.status_code in [301, 302, 303, 307, 308]:
                # æ ‡å‡†é‡å®šå‘ï¼šä» Location å¤´è·å– URL
                final_url = response.headers.get('Location')
                if not final_url:
                    # å¦‚æœæ²¡æœ‰ Location å¤´ï¼Œå¯èƒ½æ˜¯ç›¸å¯¹ URL
                    final_url = response.url
                if final_url and not final_url.startswith('http'):
                    # ç›¸å¯¹ URLï¼Œæ‹¼æ¥å®Œæ•´ URL
                    from urllib.parse import urljoin
                    final_url = urljoin(doi_url, final_url)
            elif response.status_code == 200:
                # å¯èƒ½æ˜¯é‡å®šå‘é¡µé¢ï¼ˆHTML ä¸­çš„é‡å®šå‘ï¼‰
                html_content = response.text
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ Handle Redirect é¡µé¢ï¼ˆDOI.org çš„ç‰¹æ®Šé‡å®šå‘æ ¼å¼ï¼‰
                if 'Handle Redirect' in html_content or '<a href=' in html_content:
                    soup = BeautifulSoup(html_content, 'lxml')
                    # æŸ¥æ‰¾é“¾æ¥
                    link = soup.find('a', href=True)
                    if link:
                        final_url = link.get('href')
                        if not final_url.startswith('http'):
                            from urllib.parse import urljoin
                            final_url = urljoin(doi_url, final_url)
                    else:
                        # å°è¯•ä»æ–‡æœ¬ä¸­æå– URL
                        url_match = re.search(r'https?://[^\s<>"]+', html_content)
                        if url_match:
                            final_url = url_match.group(0)
                else:
                    # ç›´æ¥æ˜¯å†…å®¹é¡µé¢
                    final_url = response.url
                    html_content = response.text
                    return self._extract_from_html_with_llm(html_content, final_url, paper_title)
            
            if not final_url:
                print(f"  âš  æ— æ³•è·å–é‡å®šå‘åçš„ URL")
                return None
            
            if config.DEBUG:
                print(f"  [DEBUG] DOI é‡å®šå‘åˆ°: {final_url}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥çš„å—ä¿æŠ¤ç½‘ç«™ï¼ˆä½† ACM å·²ç»åœ¨ä¸Šé¢å¤„ç†äº†ï¼‰
            protected_domains = ['aclanthology.org', 'ieee.org']
            final_domain = final_url.split('/')[2] if '/' in final_url else ''
            
            if any(domain in final_domain.lower() for domain in protected_domains):
                print(f"  âš  æ£€æµ‹åˆ°å— Cloudflare ä¿æŠ¤çš„ç½‘ç«™ ({final_domain})ï¼Œè·³è¿‡è®¿é—®")
                print(f"  ğŸ’¡ æç¤º: å»ºè®®ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“ï¼ˆå¦‚ DBLPã€Google Scholarï¼‰è·å–é¡µç ")
                return None
            
            # è®¿é—®å®é™…çš„ç›®æ ‡ URL
            # æ·»åŠ  Referer å¤´ï¼Œè¡¨æ˜æ¥è‡ª doi.org
            headers = {}
            if hasattr(self.web_session, 'headers'):
                headers = self.web_session.headers.copy()
            headers['Referer'] = doi_url
            headers['Origin'] = 'https://doi.org'
            
            response = self.web_session.get(
                final_url,
                timeout=config.REQUEST_TIMEOUT,
                headers=headers,
                allow_redirects=True
            )
            
            # å¦‚æœæ˜¯ 403 é”™è¯¯ï¼Œå¯èƒ½æ˜¯ Cloudflare ä¿æŠ¤
            if response.status_code == 403:
                # æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦æ˜¯ Cloudflare æŒ‘æˆ˜
                if 'just a moment' in response.text.lower() or 'checking your browser' in response.text.lower():
                    print(f"  âš  ç½‘ç«™ä½¿ç”¨äº† Cloudflare ä¿æŠ¤ï¼Œæ— æ³•è‡ªåŠ¨è®¿é—®")
                    print(f"  ğŸ’¡ æç¤º: å»ºè®®ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“ï¼ˆå¦‚ DBLPã€Google Scholarï¼‰è·å–é¡µç ")
                    return None
                
                if config.DEBUG:
                    print(f"  [DEBUG] é‡åˆ° 403 é”™è¯¯ï¼Œå°è¯•æ”¹è¿›è¯·æ±‚å¤´")
                
                # å°è¯•æ›´çœŸå®çš„æµè§ˆå™¨å¤´
                headers.update({
                    'Referer': 'https://www.google.com/',
                    'Origin': 'https://www.google.com',
                })
                
                response = self.web_session.get(
                    final_url,
                    timeout=config.REQUEST_TIMEOUT,
                    headers=headers
                )
            
            response.raise_for_status()
            
            # è§£æ HTML
            html_content = response.text
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Cloudflare æŒ‘æˆ˜é¡µé¢
            if 'just a moment' in html_content.lower() or 'checking your browser' in html_content.lower():
                print(f"  âš  ç½‘ç«™ä½¿ç”¨äº† Cloudflare ä¿æŠ¤ï¼Œæ— æ³•è‡ªåŠ¨è®¿é—®")
                print(f"  ğŸ’¡ æç¤º: å»ºè®®ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“ï¼ˆå¦‚ DBLPã€Google Scholarï¼‰è·å–é¡µç ")
                return None
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢æˆ–éœ€è¦ç™»å½•
            if 'access denied' in html_content.lower() or 'forbidden' in html_content.lower():
                print(f"  âš  ç½‘é¡µå¯èƒ½è¦æ±‚ç™»å½•æˆ–è®¿é—®è¢«æ‹’ç»")
                if config.DEBUG:
                    print(f"  [DEBUG] ç½‘é¡µå†…å®¹å‰ 500 å­—ç¬¦: {html_content[:500]}")
                return None
            
            # ä½¿ç”¨ LLM æå–ï¼ˆä¼ å…¥å®Œæ•´ HTMLï¼Œè®© LLM è‡ªå·±è§£æï¼‰
            return self._extract_from_html_with_llm(html_content, final_url, paper_title)
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                print(f"  âš  è®¿é—®è¢«æ‹’ç» (403): ç½‘ç«™å¯èƒ½é˜»æ­¢äº†è‡ªåŠ¨åŒ–è®¿é—®")
                print(f"  âš  å»ºè®®: å¯¹äº ACM ç­‰ç½‘ç«™ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ä»£ç†æˆ–æµè§ˆå™¨è®¿é—®")
            elif e.response.status_code == 404:
                print(f"  âš  ç½‘é¡µä¸å­˜åœ¨ (404)")
            else:
                print(f"  âš  HTTP é”™è¯¯ {e.response.status_code}: {e}")
            return None
        except Exception as e:
            print(f"ä» DOI URL æå–é¡µç å¤±è´¥: {e}")
            if config.DEBUG:
                import traceback
                print(f"  [DEBUG] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def _extract_from_html_with_llm(self, html_content: str, url: str, 
                                   paper_title: str = None) -> Optional[str]:
        """
        ä½¿ç”¨ LLM ä» HTML å†…å®¹ä¸­æå–é¡µç 
        
        Args:
            html_content: å®Œæ•´çš„ HTML å†…å®¹
            url: ç½‘é¡µ URL
            paper_title: è®ºæ–‡æ ‡é¢˜
            
        Returns:
            é¡µç å­—ç¬¦ä¸²
        """
        try:
            # è§£æ HTMLï¼Œæå–å…³é”®ä¿¡æ¯
            soup = BeautifulSoup(html_content, 'lxml')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style", "meta", "link", "nav", "footer", "header"]):
                script.decompose()
            
            # æå–å¯èƒ½åŒ…å«é¡µç çš„éƒ¨åˆ†
            # 1. æŸ¥æ‰¾åŒ…å« "pages" æˆ– "page" çš„å…ƒç´ 
            pages_elements = []
            for elem in soup.find_all(['div', 'span', 'p', 'td', 'li'], 
                                     string=re.compile(r'pages?|page\s*[:ï¼š]', re.I)):
                text = elem.get_text(strip=True)
                if text:
                    pages_elements.append(text)
            
            # 2. æå–ä¸»è¦æ–‡æœ¬å†…å®¹
            main_text = soup.get_text(separator=' ', strip=True)
            
            # æ„å»ºå‘é€ç»™ LLM çš„å†…å®¹
            # ä¼˜å…ˆå‘é€åŒ…å« "pages" çš„å…ƒç´ ï¼Œç„¶åå‘é€ä¸»è¦æ–‡æœ¬
            if pages_elements:
                text_content = '\n'.join(pages_elements[:10])  # æœ€å¤šå‰10ä¸ªç›¸å…³å…ƒç´ 
                if len(main_text) > 2000:
                    text_content += '\n\nä¸»è¦å†…å®¹:\n' + main_text[:5000]
            else:
                text_content = main_text[:8000]
            
            # ä½¿ç”¨ LLM æå–
            pages = self._extract_with_llm(text_content, url, paper_title)
            
            if pages:
                return pages
            
            # å¦‚æœç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œå°è¯•å‘é€æ›´å¤šä¸Šä¸‹æ–‡
            if len(main_text) > 8000:
                return self._extract_with_llm(main_text[:12000], url, paper_title)
            
            return None
            
        except Exception as e:
            print(f"LLM ä» HTML æå–é¡µç å¤±è´¥: {e}")
            return None
    
    def _get_selenium_driver(self):
        """
        è·å–æˆ–åˆ›å»º Selenium WebDriver
        
        Returns:
            WebDriver å®ä¾‹
        """
        if self.driver is not None:
            return self.driver
        
        if not SELENIUM_AVAILABLE:
            return None
        
        try:
            browser = config.SELENIUM_BROWSER.lower()
            
            if browser == 'chrome':
                options = ChromeOptions()
                if config.SELENIUM_HEADLESS:
                    options.add_argument('--headless')
                options.add_argument('--no-sandbox')
                options.add_argument('--disable-dev-shm-usage')
                options.add_argument('--disable-blink-features=AutomationControlled')
                options.add_experimental_option("excludeSwitches", ["enable-automation"])
                options.add_experimental_option('useAutomationExtension', False)
                
                # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœé…ç½®äº†ä¸”æœ‰æ•ˆï¼‰
                if config.PROXIES and isinstance(config.PROXIES, dict) and config.PROXIES:
                    try:
                        proxy_url = config.PROXIES.get('https') or config.PROXIES.get('http')
                        if proxy_url and proxy_url.strip():
                            options.add_argument(f'--proxy-server={proxy_url}')
                    except Exception:
                        pass  # ä»£ç†è®¾ç½®å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ
                
                # ä½¿ç”¨ webdriver_manager è‡ªåŠ¨ç®¡ç†é©±åŠ¨
                try:
                    service = ChromeService(ChromeDriverManager().install())
                    self.driver = webdriver.Chrome(service=service, options=options)
                except Exception:
                    # å¦‚æœè‡ªåŠ¨å®‰è£…å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç³»ç»Ÿ PATH ä¸­çš„é©±åŠ¨
                    self.driver = webdriver.Chrome(options=options)
                    
            elif browser == 'firefox':
                options = FirefoxOptions()
                if config.SELENIUM_HEADLESS:
                    options.add_argument('--headless')
                
                # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœé…ç½®äº†ä¸”æœ‰æ•ˆï¼‰
                if config.PROXIES and isinstance(config.PROXIES, dict) and config.PROXIES:
                    try:
                        proxy_url = config.PROXIES.get('https') or config.PROXIES.get('http')
                        if proxy_url and proxy_url.strip():
                            from urllib.parse import urlparse
                            parsed = urlparse(proxy_url)
                            proxy_host = parsed.hostname
                            proxy_port = parsed.port or (8080 if parsed.scheme == 'http' else 443)
                            if proxy_host:
                                options.set_preference("network.proxy.type", 1)
                                options.set_preference("network.proxy.http", proxy_host)
                                options.set_preference("network.proxy.http_port", proxy_port)
                                options.set_preference("network.proxy.ssl", proxy_host)
                                options.set_preference("network.proxy.ssl_port", proxy_port)
                    except Exception:
                        pass  # ä»£ç†è®¾ç½®å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ
                
                try:
                    service = FirefoxService(GeckoDriverManager().install())
                    self.driver = webdriver.Firefox(service=service, options=options)
                except Exception:
                    self.driver = webdriver.Firefox(options=options)
            else:
                print(f"  âš  ä¸æ”¯æŒçš„æµè§ˆå™¨: {browser}")
                return None
            
            # è®¾ç½®çª—å£å¤§å°
            self.driver.set_window_size(1920, 1080)
            
            if config.DEBUG:
                print(f"  [DEBUG] Selenium WebDriver åˆå§‹åŒ–æˆåŠŸ")
            
            return self.driver
            
        except Exception as e:
            print(f"  âš  Selenium WebDriver åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"  ğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²å®‰è£…æµè§ˆå™¨é©±åŠ¨ï¼ˆChromeDriver æˆ– GeckoDriverï¼‰")
            return None
    
    def _extract_with_selenium(self, url: str) -> Optional[str]:
        """
        ä½¿ç”¨ Selenium è®¿é—®ç½‘é¡µå¹¶è·å– HTML å†…å®¹
        
        Args:
            url: ç½‘é¡µ URL
            
        Returns:
            HTML å†…å®¹å­—ç¬¦ä¸²
        """
        if not config.USE_SELENIUM:
            return None
        
        driver = self._get_selenium_driver()
        if not driver:
            return None
        
        try:
            if config.DEBUG:
                print(f"  [DEBUG] ä½¿ç”¨ Selenium è®¿é—®: {url}")
            
            # è®¿é—®ç½‘é¡µ
            driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½ï¼ˆç­‰å¾… Cloudflare æŒ‘æˆ˜å®Œæˆæˆ–é¡µé¢å†…å®¹å‡ºç°ï¼‰
            wait = WebDriverWait(driver, config.SELENIUM_WAIT_TIME)
            
            try:
                # ç­‰å¾… Cloudflare æŒ‘æˆ˜å®Œæˆï¼ˆæ£€æµ‹åˆ°ä¸å†æœ‰ "Just a moment"ï¼‰
                wait.until_not(
                    EC.presence_of_element_located((By.XPATH, "//title[contains(text(), 'Just a moment')]"))
                )
            except TimeoutException:
                # å¦‚æœè¶…æ—¶ï¼Œå¯èƒ½ä»ç„¶åœ¨ Cloudflare æŒ‘æˆ˜é¡µé¢
                if config.DEBUG:
                    print(f"  [DEBUG] ç­‰å¾… Cloudflare æŒ‘æˆ˜è¶…æ—¶")
                pass
            
            # é¢å¤–ç­‰å¾…å‡ ç§’ï¼Œç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
            import time
            time.sleep(2)
            
            # è·å–é¡µé¢æºç 
            html_content = driver.page_source
            
            # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨ Cloudflare æŒ‘æˆ˜é¡µé¢
            if 'just a moment' in html_content.lower():
                print(f"  âš  ä»ç„¶åœ¨ Cloudflare æŒ‘æˆ˜é¡µé¢ï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´")
                # å†ç­‰å¾…ä¸€æ®µæ—¶é—´
                time.sleep(5)
                html_content = driver.page_source
            
            return html_content
            
        except TimeoutException:
            print(f"  âš  Selenium è®¿é—®è¶…æ—¶")
            return None
        except WebDriverException as e:
            print(f"  âš  Selenium WebDriver é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"  âš  Selenium è®¿é—®å¤±è´¥: {e}")
            if config.DEBUG:
                import traceback
                print(f"  [DEBUG] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def __del__(self):
        """æ¸…ç†èµ„æºï¼šå…³é—­ WebDriver"""
        if self.driver:
            try:
                self.driver.quit()
            except Exception:
                pass
