"""
æ ¸å¿ƒæœç´¢å¼•æ“
æ”¯æŒå¤šä¸ªå­¦æœ¯æ•°æ®æºçš„æœç´¢
"""
import re
import requests
import time
from typing import Optional, Dict, Any, List
from urllib.parse import quote

import config
from .extractors import extract_pages
from .utils import clean_title, similarity_score, parse_author_list


class BaseSearcher:
    """æœç´¢å¼•æ“åŸºç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': config.USER_AGENT})
        if config.PROXIES:
            self.session.proxies.update(config.PROXIES)
    
    def search(self, query: str) -> Optional[Dict[str, Any]]:
        """
        æœç´¢è®ºæ–‡
        
        Args:
            query: è®ºæ–‡æ ‡é¢˜
            
        Returns:
            è®ºæ–‡ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        raise NotImplementedError


class SemanticScholarSearcher(BaseSearcher):
    """Semantic Scholar æœç´¢å¼•æ“"""
    
    def __init__(self):
        super().__init__()
        # Semantic Scholar API
        self.base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
        self.paper_url = "https://api.semanticscholar.org/graph/v1/paper"
        
        # è®¾ç½® API Keyï¼ˆå¦‚æœæä¾›ï¼‰
        if config.SEMANTIC_SCHOLAR_API_KEY:
            self.session.headers['x-api-key'] = config.SEMANTIC_SCHOLAR_API_KEY
    
    def search(self, query: str) -> Optional[Dict[str, Any]]:
        """æœç´¢è®ºæ–‡"""
        try:
            # æ¸…ç†æŸ¥è¯¢å­—ç¬¦ä¸²
            cleaned_query = clean_title(query)
            
            # æœç´¢å‚æ•°
            params = {
                'query': cleaned_query,
                'limit': 5,  # è¿”å›å‰ 5 ä¸ªç»“æœ
                'fields': 'title,authors,year,venue,publicationVenue,citationCount,isOpenAccess,openAccessPdf,externalIds,url',
            }
            
            # å‘é€æœç´¢è¯·æ±‚
            response = self.session.get(
                self.base_url,
                params=params,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            data = response.json()
            papers = data.get('data', [])
            
            if not papers:
                return None
            
            # æ‰¾åˆ°æœ€åŒ¹é…çš„è®ºæ–‡ï¼ˆé€šè¿‡æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
            best_match = None
            best_score = 0.0
            
            for paper in papers:
                score = similarity_score(query, paper.get('title', ''))
                if score > best_score:
                    best_score = score
                    best_match = paper
            
            # å¦‚æœç›¸ä¼¼åº¦å¤ªä½ï¼Œè®¤ä¸ºæœªæ‰¾åˆ°
            if best_score < 0.3:
                return None
            
            # è·å–è¯¦ç»†ä¿¡æ¯
            return self._parse_paper_info(best_match)
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                # é€Ÿç‡é™åˆ¶
                if not config.SEMANTIC_SCHOLAR_API_KEY:
                    print(f"âš  Semantic Scholar é€Ÿç‡é™åˆ¶ï¼ˆ429ï¼‰ï¼Œå»ºè®®è®¾ç½® API Key ä»¥æé«˜é€Ÿç‡é™åˆ¶")
                else:
                    print(f"âš  Semantic Scholar é€Ÿç‡é™åˆ¶ï¼ˆ429ï¼‰ï¼Œè¯·ç¨åé‡è¯•")
            else:
                print(f"Semantic Scholar æœç´¢å¤±è´¥: {e}")
            return None
        except requests.exceptions.RequestException as e:
            print(f"Semantic Scholar æœç´¢å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"è§£æ Semantic Scholar ç»“æœå¤±è´¥: {e}")
            return None
    
    def _parse_paper_info(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æè®ºæ–‡ä¿¡æ¯"""
        # æå–åŸºæœ¬ä¿¡æ¯
        # ä¼˜å…ˆä½¿ç”¨ publicationVenue çš„ nameï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ venue
        venue = ''
        pub_venue = paper.get('publicationVenue', {})
        if isinstance(pub_venue, dict):
            # å°è¯•è·å–å¤šä¸ªå¯èƒ½çš„å­—æ®µ
            venue = pub_venue.get('name') or pub_venue.get('alternateNames', [None])[0] or venue
        
        if not venue:
            venue = paper.get('venue', '')
        
        # ä½¿ç”¨å·¥å…·å‡½æ•°æ‰©å±• venue åç§°
        from .utils import expand_venue_name
        venue_full = expand_venue_name(venue)
        
        result = {
            'title': paper.get('title', ''),
            'authors': parse_author_list(paper.get('authors', [])),
            'year': paper.get('year'),
            'venue': venue_full,  # ä½¿ç”¨æ‰©å±•åçš„å…¨å
            'url': paper.get('url', ''),
            'citation_count': paper.get('citationCount', 0),
            'source': 'semantic_scholar',
        }
        
        # æå–å¤–éƒ¨ ID
        external_ids = paper.get('externalIds', {})
        if external_ids:
            if 'DOI' in external_ids:
                result['doi'] = external_ids['DOI']
            if 'DBLP' in external_ids:
                result['dblp_id'] = external_ids['DBLP']
                result['dblp_url'] = f"https://dblp.org/rec/{external_ids['DBLP']}"
            if 'ArXiv' in external_ids:
                result['arxiv_id'] = external_ids['ArXiv']
        
        # æå–å¼€æ”¾è®¿é—® PDF
        open_access = paper.get('openAccessPdf', {})
        if open_access and open_access.get('url'):
            result['pdf_url'] = open_access['url']
        
        # æå–é¡µç 
        pages = extract_pages(paper, source='semantic_scholar')
        if pages:
            result['pages'] = pages
        else:
            # å¦‚æœæ²¡æœ‰é¡µç ï¼Œå°è¯•é€šè¿‡ paperId è·å–è¯¦ç»†ä¿¡æ¯
            paper_id = paper.get('paperId')
            if paper_id:
                pages = self._fetch_detailed_info(paper_id)
                if pages:
                    result['pages'] = pages
        
        return result
    
    def _fetch_detailed_info(self, paper_id: str) -> Optional[str]:
        """è·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯ï¼ˆå¯èƒ½åŒ…å«é¡µç ï¼‰"""
        try:
            params = {
                'fields': 'title,citation,citationStyles',
            }
            response = self.session.get(
                f"{self.paper_url}/{paper_id}",
                params=params,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            data = response.json()
            # å°è¯•ä» citation ä¸­æå–é¡µç 
            citation = data.get('citation', {})
            if citation:
                # è¿™é‡Œå¯ä»¥è§£æ BibTeX æ ¼å¼çš„å¼•ç”¨
                bibtex = citation.get('bibtex', '')
                if bibtex and 'pages' in bibtex:
                    from .extractors import BibTeXExtractor
                    extractor = BibTeXExtractor()
                    return extractor.extract_from_bibtex(bibtex)
            
        except Exception:
            pass
        
        return None


class DBLPSearcher(BaseSearcher):
    """DBLP æœç´¢å¼•æ“"""
    
    def __init__(self):
        super().__init__()
        self.base_url = "https://dblp.org/search/publ/api"
    
    def search(self, query: str) -> Optional[Dict[str, Any]]:
        """æœç´¢è®ºæ–‡"""
        try:
            cleaned_query = clean_title(query)
            
            # DBLP API å‚æ•° - å¢åŠ è¿”å›ç»“æœæ•°ä»¥ä¾¿æ‰¾åˆ°æ›´ç²¾ç¡®çš„åŒ¹é…
            params = {
                'q': cleaned_query,
                'h': 50,  # å¢åŠ è¿”å›ç»“æœæ•°ï¼Œä»¥ä¾¿æ‰¾åˆ°æ›´å¥½çš„åŒ¹é…
                'format': 'json',
            }
            
            response = self.session.get(
                self.base_url,
                params=params,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            data = response.json()
            hits = data.get('result', {}).get('hits', {}).get('hit', [])
            
            if not hits:
                if config.DEBUG:
                    print(f"  [DEBUG] DBLP æœªè¿”å›ä»»ä½•ç»“æœ")
                return None
            
            if config.DEBUG:
                print(f"  [DEBUG] DBLP å…±è¿”å› {len(hits)} ä¸ªç»“æœï¼Œå¼€å§‹åˆ†æ...")
            
            # æ‰¾åˆ°æœ€åŒ¹é…çš„ç»“æœ
            # æ”¶é›†æ‰€æœ‰å€™é€‰ç»“æœåŠå…¶å¾—åˆ†ï¼Œè®¡ç®—è¯¦ç»†åˆ†æ•°
            candidates = []
            
            for idx, hit in enumerate(hits):
                info = hit.get('info', {})
                title = info.get('title', '')
                year = info.get('year', 'N/A')
                venue = info.get('venue', 'N/A')
                
                # è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
                score = similarity_score(query, title)
                
                # é¢å¤–çš„æƒ©ç½šï¼šå¦‚æœæ ‡é¢˜æ˜æ˜¾æ¯”æŸ¥è¯¢é•¿ï¼Œé™ä½å¾—åˆ†
                query_words = len(query.split())
                title_words = len(title.split())
                word_diff = title_words - query_words
                
                # å¦‚æœæ ‡é¢˜æ¯”æŸ¥è¯¢å¤šå¾ˆå¤šè¯ï¼Œé™ä½åˆ†æ•°
                original_score = score
                if word_diff > 3:
                    score = score * 0.7  # å‡å°‘30%çš„åˆ†æ•°
                elif word_diff > 1:
                    score = score * 0.9  # å‡å°‘10%çš„åˆ†æ•°
                
                # é¢å¤–çš„å¥–åŠ±ï¼šå¦‚æœæ ‡é¢˜é•¿åº¦å’ŒæŸ¥è¯¢é•¿åº¦éå¸¸æ¥è¿‘ï¼Œå¢åŠ åˆ†æ•°
                if abs(word_diff) <= 1 and score > 0.5:
                    score = min(1.0, score * 1.1)
                
                # è°ƒè¯•è¾“å‡ºï¼šæ‰“å°æ‰€æœ‰ç»“æœ
                if config.DEBUG:
                    print(f"    [{idx+1}] {title[:60]}{'...' if len(title) > 60 else ''}")
                    print(f"        å¹´ä»½: {year}, ä¼šè®®: {venue}")
                    print(f"        åŸå§‹å¾—åˆ†: {original_score:.3f}, è°ƒæ•´åå¾—åˆ†: {score:.3f}, è¯æ•°å·®å¼‚: {word_diff}")
                
                if score > 0.0:
                    candidates.append({
                        'info': info,
                        'score': score,
                        'title': title,
                        'word_diff': word_diff,
                        'year': year,
                    })
            
            if not candidates:
                if config.DEBUG:
                    print(f"  [DEBUG] æ²¡æœ‰æœ‰æ•ˆå€™é€‰ç»“æœ")
                return None
            
            # æŒ‰å¾—åˆ†æ’åºï¼ˆå¾—åˆ†é«˜çš„åœ¨å‰ï¼‰
            candidates.sort(key=lambda x: x['score'], reverse=True)
            
            if config.DEBUG:
                print(f"\n  [DEBUG] æ’åºåçš„å‰5ä¸ªå€™é€‰:")
                for i, cand in enumerate(candidates[:5]):
                    print(f"    {i+1}. [{cand['year']}] {cand['title'][:60]}{'...' if len(cand['title']) > 60 else ''}")
                    print(f"       å¾—åˆ†: {cand['score']:.3f}, è¯æ•°å·®å¼‚: {cand['word_diff']}")
            
            # å¦‚æœå‰å‡ ä¸ªç»“æœå¾—åˆ†å¾ˆæ¥è¿‘ï¼Œä¼˜å…ˆé€‰æ‹©æ›´çŸ­ã€æ›´ç²¾ç¡®çš„æ ‡é¢˜
            best_candidate = candidates[0]
            
            # æ£€æŸ¥å‰3ä¸ªå€™é€‰ç»“æœ
            for i in range(1, min(3, len(candidates))):
                candidate = candidates[i]
                score_diff = best_candidate['score'] - candidate['score']
                
                # å¦‚æœå¾—åˆ†å·®å¼‚å¾ˆå°ï¼ˆ< 0.05ï¼‰ï¼Œä¸”å€™é€‰ç»“æœæ›´çŸ­
                if score_diff < 0.05:
                    # å¦‚æœå€™é€‰ç»“æœçš„æ ‡é¢˜æ›´çŸ­ï¼Œä¸”æ‰€æœ‰æŸ¥è¯¢è¯éƒ½åœ¨æ ‡é¢˜ä¸­
                    if candidate['word_diff'] < best_candidate['word_diff']:
                        # æ£€æŸ¥è¦†ç›–ç‡
                        query_words_set = set(query.lower().split())
                        candidate_words_set = set(candidate['title'].lower().split())
                        coverage = len(query_words_set & candidate_words_set) / len(query_words_set) if query_words_set else 0
                        
                        if coverage >= 1.0:  # æ‰€æœ‰æŸ¥è¯¢è¯éƒ½åœ¨æ ‡é¢˜ä¸­
                            if config.DEBUG:
                                print(f"  [DEBUG] é€‰æ‹©æ›´çŸ­çš„æ ‡é¢˜: {candidate['title'][:60]}")
                            best_candidate = candidate
            
            if config.DEBUG:
                print(f"\n  [DEBUG] æœ€ç»ˆé€‰æ‹©: [{best_candidate['year']}] {best_candidate['title']}")
                print(f"  [DEBUG] æœ€ç»ˆå¾—åˆ†: {best_candidate['score']:.3f}")
            
            if best_candidate['score'] < 0.3:
                if config.DEBUG:
                    print(f"  [DEBUG] å¾—åˆ†ä½äºé˜ˆå€¼ 0.3ï¼Œè¿”å› None")
                return None
            
            return self._parse_paper_info(best_candidate['info'])
            
        except requests.exceptions.RequestException as e:
            print(f"DBLP æœç´¢å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"è§£æ DBLP ç»“æœå¤±è´¥: {e}")
            return None
    
    def _parse_paper_info(self, info: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æ DBLP è®ºæ–‡ä¿¡æ¯"""
        # æå–ä½œè€…ä¿¡æ¯ï¼ˆDBLP å¯èƒ½æœ‰å¤šç§æ ¼å¼ï¼‰
        authors_raw = info.get('authors', {})
        authors_list = []
        
        if isinstance(authors_raw, list):
            # å¦‚æœ authors ç›´æ¥æ˜¯åˆ—è¡¨
            authors_list = authors_raw
        elif isinstance(authors_raw, dict):
            # å¦‚æœ authors æ˜¯å­—å…¸ï¼Œå°è¯•è·å– author å­—æ®µ
            author_value = authors_raw.get('author', [])
            if isinstance(author_value, list):
                authors_list = author_value
            elif isinstance(author_value, str):
                authors_list = [author_value]
            elif isinstance(author_value, dict):
                authors_list = [author_value]
        elif isinstance(authors_raw, str):
            # å¦‚æœ authors ç›´æ¥æ˜¯å­—ç¬¦ä¸²
            authors_list = [authors_raw]
        
        # æå–å¹¶æ‰©å±• venue åç§°
        venue = info.get('venue', '')
        from .utils import expand_venue_name
        venue_full = expand_venue_name(venue)
        
        result = {
            'title': info.get('title', ''),
            'authors': parse_author_list(authors_list),  # ä½¿ç”¨ parse_author_list å¤„ç†
            'year': info.get('year'),
            'venue': venue_full,  # ä½¿ç”¨æ‰©å±•åçš„å…¨å
            'url': info.get('ee', info.get('url', '')),
            'source': 'dblp',
        }
        
        # æå–é¡µç 
        pages = extract_pages(result, source='dblp')
        if pages:
            result['pages'] = pages
        
        return result


class CrossRefSearcher(BaseSearcher):
    """CrossRef æœç´¢å¼•æ“"""
    
    def __init__(self):
        super().__init__()
        self.base_url = "https://api.crossref.org/works"
    
    def search(self, query: str) -> Optional[Dict[str, Any]]:
        """æœç´¢è®ºæ–‡"""
        try:
            cleaned_query = clean_title(query)
            
            params = {
                'query.title': cleaned_query,
                'rows': 5,
            }
            
            response = self.session.get(
                self.base_url,
                params=params,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            data = response.json()
            items = data.get('message', {}).get('items', [])
            
            if not items:
                return None
            
            # æ‰¾åˆ°æœ€åŒ¹é…çš„ç»“æœ
            best_match = None
            best_score = 0.0
            
            for item in items:
                title = item.get('title', [])
                title_str = title[0] if title else ''
                score = similarity_score(query, title_str)
                
                if score > best_score:
                    best_score = score
                    best_match = item
            
            if best_score < 0.3:
                return None
            
            return self._parse_paper_info(best_match)
            
        except requests.exceptions.RequestException as e:
            print(f"CrossRef æœç´¢å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"è§£æ CrossRef ç»“æœå¤±è´¥: {e}")
            return None
    
    def _parse_paper_info(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æ CrossRef è®ºæ–‡ä¿¡æ¯"""
        # æå– venueï¼ˆcontainer-title é€šå¸¸æ˜¯å…¨åï¼Œä½†æˆ‘ä»¬ä¹Ÿå°è¯•æ‰©å±•ï¼‰
        venue = item.get('container-title', [])[0] if item.get('container-title') else ''
        from .utils import expand_venue_name
        venue_full = expand_venue_name(venue)
        
        # æå–å·æœŸå·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        volume = item.get('volume')
        issue = item.get('issue') or item.get('number')
        
        result = {
            'title': item.get('title', [])[0] if item.get('title') else '',
            'authors': [f"{a.get('given', '')} {a.get('family', '')}".strip() 
                       for a in item.get('author', [])],
            'year': item.get('published-print', {}).get('date-parts', [[None]])[0][0],
            'venue': venue_full,  # ä½¿ç”¨æ‰©å±•åçš„å…¨å
            'url': item.get('URL', ''),
            'doi': item.get('DOI', ''),
            'source': 'crossref',
        }
        
        # æ·»åŠ å·æœŸå·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if volume:
            result['volume'] = str(volume)
        if issue:
            result['issue'] = str(issue)
        
        # æå–é¡µç 
        pages = extract_pages(result, source='crossref')
        if pages:
            result['pages'] = pages
        
        return result


class PaperAgent:
    """æ™ºèƒ½æ–‡çŒ®é¡µç æœç´¢ Agent"""
    
    def __init__(self):
        """åˆå§‹åŒ– Agent"""
        # å»¶è¿Ÿå¯¼å…¥å…¶ä»–æœç´¢å¼•æ“ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
        try:
            from .google_scholar_searcher import GoogleScholarSearcher
            google_scholar = GoogleScholarSearcher()
        except ImportError:
            google_scholar = None
        
        try:
            from .pmlr_searcher import PMLRSearcher
            pmlr_searcher = PMLRSearcher()
        except ImportError:
            pmlr_searcher = None
        
        self.searchers = {
            'semantic_scholar': SemanticScholarSearcher(),
            'dblp': DBLPSearcher(),
            'crossref': CrossRefSearcher(),
        }
        
        # å¦‚æœ PMLR å¯ç”¨ï¼Œæ·»åŠ åˆ°æœç´¢å¼•æ“åˆ—è¡¨ï¼ˆä¼˜å…ˆï¼‰
        if pmlr_searcher:
            self.searchers['pmlr'] = pmlr_searcher
        
        # å¦‚æœ Google Scholar å¯ç”¨ï¼Œæ·»åŠ åˆ°æœç´¢å¼•æ“åˆ—è¡¨
        if google_scholar:
            self.searchers['google_scholar'] = google_scholar
        
        # å¯¼å…¥ç¼“å­˜ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
        from .cache import CacheManager
        self.cache = CacheManager()
    
    def search(self, query: str, use_cache: bool = True, 
               search_engines: List[str] = None) -> Optional[Dict[str, Any]]:
        """
        æœç´¢è®ºæ–‡å¹¶è·å–é¡µç ä¿¡æ¯
        
        Args:
            query: è®ºæ–‡æ ‡é¢˜
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            search_engines: è¦ä½¿ç”¨çš„æœç´¢å¼•æ“åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨ config.SEARCH_ENGINES
            
        Returns:
            åŒ…å«è®ºæ–‡ä¿¡æ¯å’Œé¡µç çš„å­—å…¸ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cached_result = self.cache.get(query)
            if cached_result:
                print(f"âœ“ ä»ç¼“å­˜è·å–: {query}")
                return cached_result
        
        # ç¡®å®šä½¿ç”¨çš„æœç´¢å¼•æ“
        engines = search_engines or config.SEARCH_ENGINES
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•å„ä¸ªæœç´¢å¼•æ“
        for engine_name in engines:
            if engine_name not in self.searchers:
                continue
            
            print(f"æœç´¢ä¸­ ({engine_name}): {query}...")
            
            searcher = self.searchers[engine_name]
            result = searcher.search(query)
            
            if result:
                # å¦‚æœæœ‰ DOIï¼Œä¼˜å…ˆä½¿ç”¨ doi2bib.org è·å–é¡µç 
                doi = result.get('doi')
                if doi:
                    print(f"  âœ“ æ£€æµ‹åˆ° DOI: {doi}")
                    if not result.get('pages'):
                        try:
                            from .extractors import DOI2BibExtractor
                            doi2bib_extractor = DOI2BibExtractor()
                            print(f"  å°è¯•ä½¿ç”¨ doi2bib.org è·å–é¡µç ...")
                            pages = doi2bib_extractor.extract_from_doi(doi)
                            if pages:
                                result['pages'] = pages
                                result['pages_source'] = 'doi2bib'
                                print(f"  âœ“ ä» doi2bib.org æˆåŠŸè·å–é¡µç : {pages}")
                            else:
                                print(f"  âš  doi2bib.org æœªæ‰¾åˆ°é¡µç ")
                        except Exception as e:
                            print(f"  âš  doi2bib.org æå–å¤±è´¥: {e}")
                            if config.DEBUG:
                                import traceback
                                print(f"  [DEBUG] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                    else:
                        print(f"  â„¹ å·²æœ‰é¡µç ä¿¡æ¯ï¼Œè·³è¿‡ DOI æå–")
                else:
                    if config.DEBUG:
                        print(f"  [DEBUG] æœªæ£€æµ‹åˆ° DOI")
                
                # å¦‚æœæ²¡æœ‰é¡µç ï¼Œå°è¯•å…¶ä»–æœç´¢å¼•æ“è¡¥å……
                if not result.get('pages'):
                    result = self._supplement_pages(result, engines)
                
                # ä¿å­˜åˆ°ç¼“å­˜
                if use_cache:
                    self.cache.set(query, result)
                
                return result
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        print(f"âœ— æœªæ‰¾åˆ°: {query}")
        return None
    
    def _supplement_pages(self, result: Dict[str, Any], 
                         engines: List[str]) -> Dict[str, Any]:
        """
        å¦‚æœä¸»æœç´¢å¼•æ“æœªæ‰¾åˆ°é¡µç ï¼Œå°è¯•ä»å…¶ä»–æ¥æºè¡¥å……
        
        Args:
            result: å·²æœ‰çš„è®ºæ–‡ä¿¡æ¯
            engines: å¯ç”¨çš„æœç´¢å¼•æ“åˆ—è¡¨
            
        Returns:
            æ›´æ–°åçš„è®ºæ–‡ä¿¡æ¯
        """
        url = result.get('url') or result.get('dblp_url') or result.get('pdf_url')
        paper_title = result.get('title', '')
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ NeurIPS URL
        if url and ('neurips.cc' in url.lower() or 'nips.cc' in url.lower()):
            try:
                from .neurips_extractor import NeurIPSExtractor
                print("  å°è¯•ä» NeurIPS ç½‘é¡µæå– BibTeX é¡µç ...")
                neurips_extractor = NeurIPSExtractor()
                pages = neurips_extractor.extract_from_url(url, paper_title)
                if pages:
                    result['pages'] = pages
                    result['pages_source'] = 'neurips_bibtex'
                    return result
            except Exception as e:
                print(f"  NeurIPS æå–å¤±è´¥: {e}")
        
        # å¦‚æœæœ‰ DBLP URLï¼Œå°è¯•ä» DBLP è·å–é¡µç 
        dblp_url = result.get('dblp_url') or result.get('url')
        if dblp_url and ('dblp.org' in dblp_url or 'dblp' in engines):
            print("  å°è¯•ä»ç½‘é¡µè¡¥å……é¡µç ä¿¡æ¯...")
            from .extractors import DBLPExtractor
            extractor = DBLPExtractor()
            # ä½¿ç”¨ extract æ–¹æ³•ï¼Œå®ƒä¼šè‡ªåŠ¨å°è¯•ä¼ ç»Ÿæ–¹æ³•å’Œ LLM æ–¹æ³•
            pages = extractor.extract({
                'dblp_url': dblp_url,
                'url': dblp_url,
                'title': paper_title,
            })
            if pages:
                result['pages'] = pages
                result['pages_source'] = 'web_extraction'
                return result
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ PMLR URLï¼Œå¦‚æœæ˜¯ï¼Œä½¿ç”¨ PMLR æå–å™¨
        if url and 'proceedings.mlr.press' in url.lower():
            try:
                from .pmlr_searcher import PMLRSearcher
                print("  å°è¯•ä» PMLR ç½‘é¡µæå–è¯¦ç»†ä¿¡æ¯...")
                pmlr_searcher = PMLRSearcher()
                pmlr_result = pmlr_searcher._extract_from_pmlr_url(url, paper_title)
                if pmlr_result:
                    # è¡¥å……é¡µç ç­‰ä¿¡æ¯
                    if pmlr_result.get('pages') and not result.get('pages'):
                        result['pages'] = pmlr_result['pages']
                        result['pages_source'] = 'pmlr'
                    
                    # è¡¥å…… BibTeXï¼ˆå¦‚æœæœ‰ï¼‰
                    if pmlr_result.get('bibtex') and not result.get('bibtex'):
                        result['bibtex'] = pmlr_result['bibtex']
            except Exception as e:
                if config.DEBUG:
                    print(f"  [DEBUG] PMLR æå–å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰é¡µç ï¼Œä¼˜å…ˆå¤„ç† DOI URL
        if not result.get('pages') and url:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å— Cloudflare ä¸¥æ ¼ä¿æŠ¤çš„ç½‘ç«™ï¼ˆç›´æ¥è·³è¿‡ï¼‰
            protected_domains = ['dl.acm.org', 'aclanthology.org', 'ieee.org']
            is_protected = any(domain in url.lower() for domain in protected_domains)
            
            if is_protected:
                print(f"  âš  æ£€æµ‹åˆ°å— Cloudflare ä¿æŠ¤çš„ç½‘ç«™ï¼Œè·³è¿‡è‡ªåŠ¨æå–")
                print(f"  ğŸ’¡ æç¤º: å¯¹äº {url.split('/')[2]}ï¼Œå»ºè®®ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“ï¼ˆå¦‚ DBLPã€Google Scholarï¼‰è·å–é¡µç ")
                return result
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ DOI URL
            if 'doi.org' in url.lower() or url.startswith('https://doi.org/') or url.startswith('http://doi.org/'):
                # é¦–å…ˆå°è¯•ä½¿ç”¨ doi2bib.orgï¼ˆæ›´å¿«é€Ÿã€å¯é ï¼‰
                try:
                    # æå– DOI æ ‡è¯†ç¬¦
                    doi_match = re.search(r'doi\.org/([^/]+/?.*)', url)
                    if doi_match:
                        doi_identifier = doi_match.group(1).rstrip('/')
                        
                        # ä½¿ç”¨ doi2bib.org è·å– BibTeX å¹¶æå–é¡µç 
                        from .extractors import DOI2BibExtractor
                        doi2bib_extractor = DOI2BibExtractor()
                        print(f"  å°è¯•ä½¿ç”¨ doi2bib.org è·å–é¡µç ...")
                        pages = doi2bib_extractor.extract_from_doi(doi_identifier)
                        
                        if pages:
                            result['pages'] = pages
                            result['pages_source'] = 'doi2bib'
                            return result
                        else:
                            if config.DEBUG:
                                print(f"  [DEBUG] doi2bib.org æœªæ‰¾åˆ°é¡µç ")
                except Exception as e:
                    if config.DEBUG:
                        print(f"  [DEBUG] doi2bib.org æå–å¤±è´¥: {e}")
                
                # å¦‚æœ doi2bib.org å¤±è´¥ï¼Œå°è¯• LLM æå–
                try:
                    from .llm_extractor import LLMExtractor
                    llm_extractor = LLMExtractor()
                    
                    # å°è¯•è·å–é‡å®šå‘åçš„ URLï¼ˆä¸è®¿é—®å†…å®¹ï¼‰
                    import requests
                    response = requests.head(url, allow_redirects=True, timeout=5)
                    redirected_url = response.url
                    
                    # æ£€æŸ¥é‡å®šå‘åçš„åŸŸå
                    redirected_domain = redirected_url.split('/')[2] if '/' in redirected_url else ''
                    if any(domain in redirected_domain.lower() for domain in protected_domains):
                        print(f"  âš  DOI é‡å®šå‘åˆ°å—ä¿æŠ¤çš„ç½‘ç«™ ({redirected_domain})ï¼Œè·³è¿‡ LLM æå–")
                        print(f"  ğŸ’¡ æç¤º: å»ºè®®ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“ï¼ˆå¦‚ DBLPã€Google Scholarï¼‰è·å–é¡µç ")
                        return result
                    
                    # å¦‚æœä¸å—ä¿æŠ¤ï¼Œå°è¯•æå–
                    print(f"  å°è¯•ä½¿ç”¨ LLM ä» DOI ç½‘é¡µæå–é¡µç ...")
                    pages = llm_extractor.extract_from_doi_url(url, paper_title)
                    if pages:
                        result['pages'] = pages
                        result['pages_source'] = 'llm_doi_extraction'
                        return result
                    else:
                        # DOI æå–å¤±è´¥
                        print(f"  âš  DOI ç½‘é¡µæå–å¤±è´¥")
                except Exception as e:
                    if config.DEBUG:
                        print(f"  [DEBUG] DOI LLM æå–å¤±è´¥: {e}")
            
            # å¦‚æœä¸æ˜¯ DOI URL æˆ–å…¶ä»–æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•é€šç”¨ LLM æå–
            # ä½†è·³è¿‡å·²çŸ¥çš„å—ä¿æŠ¤ç½‘ç«™ï¼ˆé¿å…é‡å¤å¤±è´¥ï¼‰
            if not any(domain in url.lower() for domain in protected_domains):
                try:
                    from .llm_extractor import LLMExtractor
                    llm_extractor = LLMExtractor()
                    print(f"  å°è¯•ä½¿ç”¨ LLM ä»ç½‘é¡µæå–é¡µç ...")
                    pages = llm_extractor.extract_from_url(url, paper_title)
                    if pages:
                        result['pages'] = pages
                        result['pages_source'] = 'llm_extraction'
                except Exception:
                    pass
        
        return result
    
    def batch_search(self, queries: List[str], 
                    use_cache: bool = True) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æœç´¢
        
        Args:
            queries: è®ºæ–‡æ ‡é¢˜åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            ç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ªæŸ¥è¯¢ä¸€ä¸ªç»“æœï¼‰
        """
        results = []
        for query in queries:
            result = self.search(query, use_cache=use_cache)
            results.append({
                'query': query,
                'result': result,
            })
        return results

